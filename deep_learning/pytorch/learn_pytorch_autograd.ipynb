{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c711dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一个简单的例子\n",
    "import torch\n",
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "y = torch.tensor([10.0, 20.0, 30.0], requires_grad=True)\n",
    "xx = x * x\n",
    "yy = y * y\n",
    "xy = x * y\n",
    "xx.retain_grad()  # 保留非叶子张量的梯度\n",
    "yy.retain_grad()\n",
    "xy.retain_grad()\n",
    "z = 10 * (xx + yy + 2 * xy)\n",
    "z.retain_grad()\n",
    "s = z.sum()\n",
    "s.backward()\n",
    "print('x.grad:\\n ', x.grad)\n",
    "print('y.grad:\\n ', y.grad)\n",
    "print('xx.grad:\\n ', xx.grad)\n",
    "print('yy.grad:\\n ', yy.grad)\n",
    "print('xy.grad:\\n ', xy.grad)\n",
    "print('z.grad:\\n ', z.grad)  # tensor([1., 1., 1.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6619328f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 也可以直接对 `z` 调用 `backward`，这将会触发从 `z` 开始的整个计算图的反向传播。\n",
    "# 不过需要注意的是，在计算图中，只有当计算结果是一个标量（单个数值）时调用 `backward()` 最为常用。\n",
    "# 因为如果张量包含多个元素时，在调用 `backward` 时必须明确指定它要传播的梯度（通常被称为 `gradient` 参数）。\n",
    "# 在上面的例子中，`z` 是一个包含多个元素的张量，因此在调用 `z.backward()` 时，需要指定 `gradient` 参数。\n",
    "import torch\n",
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "y = torch.tensor([10.0, 20.0, 30.0], requires_grad=True)\n",
    "xx = x * x\n",
    "yy = y * y\n",
    "xy = x * y\n",
    "xx.retain_grad()  # 保留非叶子张量的梯度\n",
    "yy.retain_grad()\n",
    "xy.retain_grad()\n",
    "z = 10 * (xx + yy + 2 * xy)\n",
    "z.retain_grad()\n",
    "z.backward(torch.ones_like(z))  # 直接对 z 调用 backward，并指定 gradient 参数。此处使用 torch.ones_like(z) 等效于前一例子中使用 s = z.sum()\n",
    "                                # 计算 s.backward() 的结果\n",
    "print('x.grad:\\n ', x.grad)\n",
    "print('y.grad:\\n ', y.grad)\n",
    "print('xx.grad:\\n ', xx.grad)\n",
    "print('yy.grad:\\n ', yy.grad)\n",
    "print('xy.grad:\\n ', xy.grad)\n",
    "print('z.grad:\\n ', z.grad)  # tensor([1., 1., 1.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04c5ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对于非 s = z.sum() 的情况\n",
    "import torch\n",
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "y = torch.tensor([10.0, 20.0, 30.0], requires_grad=True)\n",
    "xx = x * x\n",
    "yy = y * y\n",
    "xy = x * y\n",
    "xx.retain_grad()  # 保留非叶子张量的梯度\n",
    "yy.retain_grad()\n",
    "xy.retain_grad()\n",
    "z = 10 * (xx + yy + 2 * xy)\n",
    "z.retain_grad()\n",
    "s = z.sum() * 0.3  # 为了区别于第一个例子，我们使 s = z.sum() * 0.3\n",
    "s.backward()\n",
    "print('x.grad:\\n ', x.grad)\n",
    "print('y.grad:\\n ', y.grad)\n",
    "print('xx.grad:\\n ', xx.grad)\n",
    "print('yy.grad:\\n ', yy.grad)\n",
    "print('xy.grad:\\n ', xy.grad)\n",
    "print('z.grad:\\n ', z.grad)  # tensor([0.3., 0.3., 0.3.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ae8341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对于 s = z.sum() * 0.3, 如果直接从 z 计算梯度，则需要将 z.backward 函数的参数 gradient 乘 0.3\n",
    "import torch\n",
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "y = torch.tensor([10.0, 20.0, 30.0], requires_grad=True)\n",
    "xx = x * x\n",
    "yy = y * y\n",
    "xy = x * y\n",
    "xx.retain_grad()  # 保留非叶子张量的梯度\n",
    "yy.retain_grad()\n",
    "xy.retain_grad()\n",
    "z = 10 * (xx + yy + 2 * xy)\n",
    "z.retain_grad()\n",
    "z.backward(torch.ones_like(z) * 0.3)  # 直接对 z 调用 backward，并指定 gradient 参数。此处使用 torch.ones_like(z) * 0.3 等效于前一例子中使用 s = z.sum() * 0.3\n",
    "                                # 计算 s.backward() 的结果\n",
    "print('x.grad:\\n ', x.grad)\n",
    "print('y.grad:\\n ', y.grad)\n",
    "print('xx.grad:\\n ', xx.grad)\n",
    "print('yy.grad:\\n ', yy.grad)\n",
    "print('xy.grad:\\n ', xy.grad)\n",
    "print('z.grad:\\n ', z.grad)  # tensor([0.3., 0.3., 0.3.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3ee6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 torch.autograd.Function 复刻最初的例子\n",
    "import torch\n",
    "\n",
    "class CustomFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, y):\n",
    "        # 计算前向结果，并保存必要的上下文信息，供后向传播时使用\n",
    "        xx = x * x\n",
    "        yy = y * y\n",
    "        xy = x * y\n",
    "        z = 10 * (xx + yy + 2 * xy)\n",
    "        \n",
    "        # 保存中间结果\n",
    "        ctx.save_for_backward(x, y)\n",
    "        \n",
    "        return z\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # 从上下文中恢复前向传播时保存的变量\n",
    "        x, y = ctx.saved_tensors\n",
    "        \n",
    "        # 计算每个中间变量的梯度\n",
    "        grad_x = grad_output * (20 * x + 20 * y)\n",
    "        grad_y = grad_output * (20 * y + 20 * x)\n",
    "        \n",
    "        # 返回与输入张量数量相同的梯度\n",
    "        return grad_x, grad_y\n",
    "\n",
    "# 测试自定义的 autograd.Function\n",
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "y = torch.tensor([10.0, 20.0, 30.0], requires_grad=True)\n",
    "\n",
    "# 使用自定义函数进行前向传播\n",
    "z = CustomFunction.apply(x, y)\n",
    "\n",
    "# 调用backward方法，计算梯度\n",
    "z.sum().backward()\n",
    "\n",
    "# 打印梯度结果\n",
    "print('x.grad:\\n ', x.grad)\n",
    "print('y.grad:\\n ', y.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e09f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义的 autograd.Function 的 forward 方法可以输出多个张量。\n",
    "# 若 forward 输出了多个张量，backward 也需要增加参数，位于 ctx 这个参数后的参数与 forward 方法输出的多个张量的梯度一一对应。\n",
    "# 这个例子中虽然 backward 函数接受了多个梯度参数，但除了第一个梯度参数外，其余的都没有被使用\n",
    "\n",
    "# 扩展前向函数的输出以包括中间变量\n",
    "class ExtendedCustomFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, y):\n",
    "        xx = x * x\n",
    "        yy = y * y\n",
    "        xy = x * y\n",
    "        z = 10 * (xx + yy + 2 * xy)\n",
    "        \n",
    "        # 保存上下文\n",
    "        ctx.save_for_backward(x, y)\n",
    "        \n",
    "        return z, xx, yy, xy  # 除了输出 z 外，同时输出 xx yy xy，用于打印\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_z, grad_xx, grad_yy, grad_xy):  # grad_z, grad_xx, grad_yy, grad_xy 对应于 forward 的输出张量。\n",
    "                                                           # 由于我们只计算 dz/dx 与 dz/dy，所以下面的计算中只用到了 grad_z\n",
    "        x, y = ctx.saved_tensors\n",
    "        \n",
    "        grad_x = grad_z * (20 * x + 20 * y)\n",
    "        grad_y = grad_z * (20 * y + 20 * x)\n",
    "        \n",
    "        # 返回梯度，确保每个变量的梯度都计算并返回\n",
    "        return grad_x, grad_y\n",
    "\n",
    "# 测试自定义的 autograd.Function\n",
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "y = torch.tensor([10.0, 20.0, 30.0], requires_grad=True)\n",
    "\n",
    "# 使用自定义函数进行前向传播\n",
    "z, xx, yy, xy = ExtendedCustomFunction.apply(x, y)\n",
    "print('z:\\n ', z)\n",
    "print('xx:\\n ', xx)\n",
    "print('yy:\\n ', yy)\n",
    "print('xy:\\n ', xy)\n",
    "\n",
    "# 调用 backward 方法，计算梯度\n",
    "z.sum().backward()\n",
    "\n",
    "# 打印梯度结果\n",
    "print('x.grad:\\n', x.grad)\n",
    "print('y.grad:\\n', y.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255d81e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义的 autograd.Function 的 backward 方法接受多个梯度参数的另一个例子。\n",
    "# 当 forward 方法返回多个张量时，每个张量在反向传播时都需要计算梯度，backward 方法接受的多个梯度参数就是对这些张量的梯度。\n",
    "\n",
    "import torch\n",
    "\n",
    "class MultipleOutputsFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        y1 = x * x\n",
    "        y2 = x * 3\n",
    "        ctx.save_for_backward(x)\n",
    "        return y1, y2\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_y1, grad_y2):\n",
    "        x, = ctx.saved_tensors\n",
    "        grad_x = grad_y1 * 2 * x + grad_y2 * 3\n",
    "        return grad_x\n",
    "\n",
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "y1, y2 = MultipleOutputsFunction.apply(x)\n",
    "z = y1 + 10 * y2\n",
    "z.sum().backward()\n",
    "print('x.grad:\\n ', x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684c770d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义 torch.autograd.Function，并在 forward 方法中加入非张量类型入参用于设置 forward 与 backward 函数的行为。\n",
    "# 虽然这些非张量参数在自动微分过程中不会被追踪和求导，但它们可以通过上下文传递在反向传播中使用。\n",
    "\n",
    "import torch\n",
    "from typing import NamedTuple\n",
    "\n",
    "class CustomSettings(NamedTuple):\n",
    "    print_debug_info: bool\n",
    "\n",
    "class CustomFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, y, custom_settings):  # 非张量类型参数 custem_settings 用于控制 forward 和 backward 函数的行为\n",
    "        # 计算前向结果，并保存必要的上下文信息，供后向传播时使用\n",
    "        xx = x * x\n",
    "        yy = y * y\n",
    "        xy = x * y\n",
    "        z = 10 * (xx + yy + 2 * xy)\n",
    "        \n",
    "        if custom_settings.print_debug_info:\n",
    "            print(f\"debug info: xx={xx}, yy={yy}, xy={xy}\")\n",
    "        \n",
    "        # 保存中间结果\n",
    "        ctx.save_for_backward(x, y)\n",
    "        ctx.custom_settings = custom_settings\n",
    "        \n",
    "        return z\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # 从上下文中恢复前向传播时保存的变量\n",
    "        x, y = ctx.saved_tensors\n",
    "        custom_settings = ctx.custom_settings\n",
    "        \n",
    "        if custom_settings.print_debug_info:\n",
    "            print(f\"debug info: grad_output={grad_output}\")\n",
    "        \n",
    "        # 计算每个中间变量的梯度\n",
    "        grad_x = grad_output * (20 * x + 20 * y)\n",
    "        grad_y = grad_output * (20 * y + 20 * x)\n",
    "        \n",
    "        # custem_settings 位置返回 None\n",
    "        return grad_x, grad_y, None\n",
    "\n",
    "# 测试自定义的 autograd.Function\n",
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "y = torch.tensor([10.0, 20.0, 30.0], requires_grad=True)\n",
    "\n",
    "custem_settings = CustomSettings(\n",
    "    print_debug_info = True,\n",
    ")\n",
    "\n",
    "# 使用自定义函数进行前向传播\n",
    "z = CustomFunction.apply(x, y, custem_settings)\n",
    "\n",
    "# 调用backward方法，计算梯度\n",
    "z.sum().backward()\n",
    "\n",
    "# 打印梯度结果\n",
    "print('x.grad:\\n ', x.grad)\n",
    "print('y.grad:\\n ', y.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78062a9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
