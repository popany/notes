# Basic Concepts in Gymnasium

- [Basic Concepts in Gymnasium](#basic-concepts-in-gymnasium)
  - [动作空间与观察空间](#动作空间与观察空间)
    - [技术解释](#技术解释)
  - [Wrapper(包装器)](#wrapper包装器)
    - [技术解释](#技术解释-1)
  - [Q-learning](#q-learning)

## 动作空间与观察空间

每个环境都通过`action_space`和`observation_space`属性指定有效动作和观察的格式。这对于了解环境的预期输入和输出非常有帮助，因为所有有效的动作和观察都应包含在它们各自的空间中。

### 技术解释

- **动作空间(action_space)**：定义了智能体可以执行的所有合法动作的集合。常见的类型包括：
  
  - **离散动作空间(Discrete)**：有限数量的离散动作，如上下左右移动、开火等
  - **连续动作空间(Box)**：连续范围内的动作值，如机器人关节的角度或力量
  - **复合动作空间(Dict, Tuple)**：由多个简单空间组合而成的复杂动作空间

- **观察空间(observation_space)**：定义了环境可能返回给智能体的所有可能观察的集合。观察表示智能体对环境当前状态的感知，可能是：
  
  - 低维数值(如位置、速度)
  - 高维图像数据(如游戏屏幕截图)
  - 结构化数据(如游戏状态的字典表示)

- **空间采样(space.sample())**：
  - `env.action_space.sample()`是一个便捷方法，可以从动作空间随机采样一个合法动作
  - 这在测试环境或实现简单的随机基线策略时非常有用
  - 在实际应用中，会用智能体策略(policy)替代随机采样

- **智能体策略(agent policy)**：
  - 策略是一个从观察到动作的映射函数
  - 在强化学习中，我们通常训练一个策略来最大化累积奖励
  - 策略可以是简单的映射表，也可以是复杂的神经网络

了解环境的动作和观察空间对于设计适合该环境的智能体架构和算法至关重要，它们定义了智能体与环境交互的"接口规范"。

## Wrapper(包装器)

Wrapper(包装器)是一种修改现有环境的便捷方式，无需直接更改底层代码。使用包装器可以帮助你避免编写大量样板代码，并使你的环境更加模块化。包装器还可以链式组合，以结合它们的效果。通过`gymnasium.make()`生成的大多数环境默认已经被`TimeLimit`、`OrderEnforcing`和`PassiveEnvChecker`包装器包装。

### 技术解释

- **包装器(Wrapper)**：
  - 包装器是一个特殊的环境类，它包含另一个环境并修改其某些行为
  - 包装器通过委托模式工作：它接收请求，可能修改这些请求，然后将它们转发给被包装的环境
  - 包装器也可以拦截和修改从被包装环境返回的响应

- **包装器的主要优势**：
  - **模块化**：每个包装器专注于一个特定的修改，使代码更清晰
  - **可重用性**：包装器可以应用于多个不同的环境
  - **可组合性**：多个包装器可以链式组合，形成复杂的修改

- **常见的默认包装器**：
  - **TimeLimit**：为环境添加时间步数限制，防止智能体在单个回合中运行过长时间
  - **OrderEnforcing**：确保用户正确调用环境方法的顺序(如必须先调用`reset()`再调用`step()`)
  - **PassiveEnvChecker**：检查环境是否符合Gymnasium API的规范，在检测到问题时发出警告

- **包装器的常见用途**：
  - 修改观察(如调整图像大小、归一化数值)
  - 修改奖励(如奖励裁剪、奖励缩放)
  - 修改动作(如动作重映射)
  - 添加额外功能(如视频录制、性能监控)
  - 修改环境逻辑(如跳帧、添加黏性动作)

通过理解和使用包装器，你可以灵活地定制环境行为，而无需从头开始创建新环境，这大大提高了强化学习实验的开发效率和可维护性。

## Q-learning

Q-learning是一种经典的强化学习算法，特别适合离散状态和动作空间的问题：

- **基本原理**：学习一个Q表格(Q-table)，存储每个状态-动作对的预期累积奖励
- **核心公式**：
  
  $$Q(s,a) \leftarrow Q(s,a) + \alpha [r + \gamma \max_{a'} Q(s',a') - Q(s,a)]$$

  其中：
  - $Q(s,a)$ 是在状态 $s$ 下采取动作 $a$ 的Q值
  - $\alpha$ 是学习率
  - $r$ 是即时奖励
  - $\gamma$ 是折扣因子
  - $s'$ 是下一个状态
  - $\max_{a'} Q(s',a')$ 是下一状态可能的最大Q值

- **探索与利用**：通常使用ε-贪心策略平衡探索和利用
  - 以 $\varepsilon$ 的概率随机选择动作(探索)
  - 以 $1-\varepsilon$ 的概率选择Q值最大的动作(利用)


Q-learning是由Watkins在1989年提出的一种无模型(model-free)、离策略(off-policy)学习算法，专为具有离散动作空间的环境设计，因其是首个在特定条件下证明能够收敛到最优策略的强化学习算法而闻名。

- **无模型(Model-free)**：
  - Q-learning不需要明确了解或构建环境模型(即状态转移概率和奖励函数)
  - 它直接从与环境的交互中学习，而不需要预先知道动作的后果
  - 这使得Q-learning能够应用于复杂或难以精确建模的环境

- **离策略(Off-policy)**：
  - "离策略"指的是学习过程可以使用与评估策略不同的行为策略
  - Q-learning可以从任何产生足够探索的策略(如ε-贪心)收集的经验中学习最优策略
  - 这与"在策略"(on-policy)算法(如SARSA)形成对比，后者必须使用它正在评估的相同策略来收集数据

- **离散动作空间**：
  - 传统Q-learning设计用于处理有限数量的离散动作
  - 这使其特别适合如21点、网格世界或Atari游戏等环境
  - 对于连续动作空间，通常需要使用Q-learning的扩展版本或其他算法

- **收敛性质**：
  - Q-learning的理论意义在于，它是首个被证明在特定条件下可以收敛到最优策略的强化学习算法
  - 这些条件包括：
    - 足够的探索(所有状态-动作对被无限次访问)
    - 适当的学习率调度(满足Robbins-Monro条件)
    - 离散有限的状态和动作空间
    - 马尔可夫性质(当前状态包含决策所需的所有信息)

- **算法核心思想**：
  - 通过迭代更新Q表格来估计每个状态-动作对的长期价值
  - Q值代表在给定状态下采取特定动作，然后遵循最优策略所能获得的预期累积奖励
  - 学习过程使用时序差分(TD)学习，结合实际获得的即时奖励和对未来状态价值的估计

Q-learning的简单性与强大的理论保证使其成为强化学习领域的基石算法，为许多现代算法(如深度Q网络DQN)奠定了基础。





