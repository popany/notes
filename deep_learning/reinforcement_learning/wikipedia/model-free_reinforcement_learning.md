# [Model-free (reinforcement learning)](https://en.wikipedia.org/wiki/Model-free_(reinforcement_learning))

- [Model-free (reinforcement learning)](#model-free-reinforcement-learning)
  - [Q\&A](#qa)
    - [无模型强化学习与在线策略还有离线策略是什么关系？](#无模型强化学习与在线策略还有离线策略是什么关系)

在强化学习(RL)中，**无模型算法**(model-free algorithm)是指不估计与马尔可夫决策过程(MDP)相关的转移概率分布(和奖励函数)的算法。

- 在强化学习中，MDP代表要解决的问题
- 转移概率分布(或转移模型)和奖励函数通常被统称为环境(或MDP)的"模型"
- 因此这类算法被称为"无模型"

无模型强化学习算法可以被视为"显式"的试错算法。它们不构建环境的内部表示，而是直接从经验中学习最优策略。

无模型算法的代表包括：
- 蒙特卡洛(MC)强化学习
- SARSA
- Q-learning

这些算法都是通过直接与环境交互并从得到的奖励信号中学习，而不是通过建模环境动态来进行决策。

蒙特卡洛(Monte Carlo, MC)估计是许多无模型强化学习算法的核心组件。MC学习算法本质上是广义策略迭代的重要分支，包含两个周期性交替的步骤：

1. **策略评估**(Policy Evaluation, PEV)
2. **策略改进**(Policy Improvement, PIM)

在此框架中：
- 首先通过对应的值函数评估每个策略
- 然后，基于评估结果，完成贪婪搜索以产生更好的策略

MC估计主要应用于第一步的策略评估：
- 使用最简单的思想来判断当前策略的有效性：平均所有已收集样本的回报
- 随着经验的累积，根据大数定律，估计将收敛到真实值
- **关键优势**：MC策略评估不需要环境动态的任何先验知识

MC只需要从与环境交互中生成的经验(即状态、动作和奖励的样本)，这种环境可以是真实的或模拟的。这使得MC成为一种真正的"从经验中学习"的方法。

值函数估计对于无模型强化学习算法至关重要。相比蒙特卡洛方法，**时间差分**(Temporal Difference, TD)方法通过重用现有值估计来学习这一函数。

TD学习具有以下关键能力：
- 能够从不完整的事件序列中学习，无需等待最终结果
- 可以将未来回报近似为当前状态的函数
- 类似MC，只使用经验来估计值函数，无需环境动态的先验知识

TD学习的优势在于可以基于其当前估计更新值函数(即"引导"特性)。因此：
- TD学习算法可以以**逐步方式**从不完整情节或持续任务中学习
- 而MC必须以**情节对情节**的方式实现

这种能够进行增量更新的特性使TD方法在很多场景中比MC方法更加实用和高效。

## Q&A

### 无模型强化学习与在线策略还有离线策略是什么关系？

无模型强化学习 (Model-free RL) 与在线策略 (On-policy) 和离线策略 (Off-policy) 是强化学习领域中 **不同维度** 的概念，但它们之间存在着紧密的联系。  理解它们之间的关系，有助于更清晰地认识强化学习算法的分类和特性。

**核心观点:**

* **无模型强化学习** 是 **相对于模型基强化学习** 的一种分类，区分了学习方法是否显式构建环境模型。
* **在线策略和离线策略** 是 **相对于学习策略和数据生成策略** 的一种分类，区分了这两个策略是否相同。
* **在线策略和离线策略的概念可以应用于模型基强化学习，但它们在无模型强化学习中更为重要和常见。**  大多数被广泛讨论的在线策略和离线策略算法都属于无模型强化学习范畴。

**具体关系解释:**

1. **无模型强化学习是更大的范畴，在线策略和离线策略是其内部的子类。**

   你可以把强化学习方法想象成一个大的集合，首先可以根据是否学习环境模型分成两大类：模型基强化学习和无模型强化学习。  然后，在 **无模型强化学习** 这个类别下，又可以根据策略学习方式进一步细分为 **在线策略** 和 **离线策略** 方法。

   ```
   强化学习 (Reinforcement Learning)
   ├── 模型基强化学习 (Model-based RL)
   │   └── (策略学习方式，可以有在线或离线策略的思路，但讨论较少)
   └── 无模型强化学习 (Model-free RL)
       ├── 在线策略 (On-policy)
       │   └── 例如: SARSA, A2C, A3C, Policy Gradient (REINFORCE, PPO, TRPO)
       └── 离线策略 (Off-policy)
           └── 例如: Q-Learning, DQN, DDPG, SAC
   ```

2. **在线策略和离线策略的区分在无模型强化学习中更加关键。**

   在模型基强化学习中，由于智能体已经学习了一个环境模型，它可以利用这个模型进行规划，策略的改进可能更多地依赖于模型的准确性和规划算法的有效性，而不太强调在线策略和离线策略的区别。

   然而，在 **无模型强化学习** 中，由于智能体**没有环境模型**作为指导，它完全依赖于与环境的 **直接交互经验** 来学习策略。  因此，**数据是如何产生的 (由哪个策略生成的数据)**，以及 **用这些数据来改进哪个策略 (学习哪个策略)**，就变得至关重要。  这直接决定了算法的学习效率、稳定性和适用场景。

3. **无模型强化学习算法可以是在线策略的，也可以是离线策略的。**

   * **无模型在线策略算法:**  使用 **当前策略** 与环境交互收集数据，并用这些数据 **改进同一个当前策略**。 典型的例子包括：
      * **SARSA:**  使用当前策略选择动作，并使用实际执行的动作、获得的奖励和下一个状态来更新当前策略的Q函数。
      * **Policy Gradient 方法 (REINFORCE, PPO, TRPO):**  使用当前策略采样轨迹，并使用这些轨迹来计算策略梯度，从而更新当前策略的参数。
      * **A2C, A3C:**  也是在线策略的 Actor-Critic 方法，使用当前策略与环境交互，并用收集的数据同时更新 Actor (策略) 和 Critic (价值函数)。

   * **无模型离线策略算法:**  可以使用 **行为策略 (Behavior Policy)** 生成的数据，来学习和改进 **目标策略 (Target Policy)**，这两个策略可以是不同的。  典型的例子包括：
      * **Q-Learning:**  虽然在执行动作时可能使用 ε-贪婪策略 (行为策略)，但在更新 Q 函数时，它假设遵循的是贪婪策略 (目标策略)，即选择 Q 值最大的动作。  行为策略和目标策略不一致，因此是离线策略。
      * **Deep Q-Network (DQN):**  同样是离线策略，利用经验回放机制存储由行为策略生成的数据，并从中采样学习目标策略。
      * **DDPG, SAC:**  也是离线策略的 Actor-Critic 方法，Actor (目标策略) 的更新可以利用 Critic (价值函数) 从行为策略生成的数据中学习。
