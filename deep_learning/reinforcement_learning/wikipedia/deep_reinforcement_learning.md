# [Deep reinforcement learning](https://en.wikipedia.org/wiki/Deep_reinforcement_learning)

- [Deep reinforcement learning](#deep-reinforcement-learning)
  - [Overview](#overview)
    - [Deep learning](#deep-learning)
    - [Reinforcement learning](#reinforcement-learning)
    - [Deep reinforcement learning](#deep-reinforcement-learning-1)
  - [History](#history)
  - [Algorithms](#algorithms)

深度强化学习(Deep Reinforcement Learning，深度RL)是机器学习的一个子领域，它结合了强化学习(RL)和深度学习。强化学习研究计算智能体如何通过试错来学习决策的问题。深度RL将深度学习纳入解决方案中，使智能体能够从非结构化输入数据中做出决策，而无需手动设计状态空间。

深度RL算法能够处理非常大的输入(例如，视频游戏中渲染到屏幕上的每一个像素)，并决定执行哪些动作来优化目标(例如，最大化游戏分数)。

深度强化学习已被应用于多种领域，包括但不限于：

- 机器人技术
- 视频游戏
- 自然语言处理
- 计算机视觉[1]
- 教育
- 交通
- 金融
- 医疗保健[2]

## Overview

### Deep learning

深度学习是机器学习的一种形式，它利用神经网络将一组输入通过人工神经网络转换为一组输出。深度学习方法，通常使用带标记数据集的监督学习，已被证明能够解决涉及处理复杂、高维原始输入数据(如图像)的任务，且比之前的方法需要更少的手动特征工程，从而使计算机视觉和自然语言处理等多个领域取得了显著进步。

在过去的十年中，深度强化学习已经在一系列问题上取得了显著成果，包括：

- 单人和多人游戏，如：
  - 围棋(Go)
  - Atari游戏
  - Dota 2
- 机器人技术

这些成就展示了深度强化学习将深度学习的表示能力与强化学习的决策框架相结合所带来的强大潜力。

### Reinforcement learning

强化学习是一个过程，智能体通过试错来学习决策。这个问题通常被数学地建模为**马尔可夫决策过程**(MDP)，其中：

- 智能体在每个时间步处于状态 $s$
- 采取动作 $a$
- 接收标量奖励
- 根据环境动态 $p(s'|s,a)$ 转移到下一个状态 $s'$

智能体试图学习一个策略 $\pi(a|s)$，即从观察到行动的映射，以最大化其回报(奖励总和的期望值)。

在强化学习中(与最优控制不同)，算法只能通过采样来访问环境动态 $p(s'|s,a)$，而不能直接获得其完整描述。

### Deep reinforcement learning

在许多实际决策问题中，MDP的状态 $s$ 是高维的，例如：
- 相机捕获的图像
- 机器人的原始传感器数据流

这类高维问题无法通过传统的强化学习算法有效解决，因为传统算法在状态空间巨大时面临"维度灾难"问题。

深度强化学习算法通过整合深度学习来解决此类MDP，通常：

- 将策略 $\pi(a|s)$ 或其他学习函数表示为**神经网络**
- 开发专门的算法，使其在这种环境中表现良好
- 利用神经网络强大的特征提取和表示能力来处理原始高维输入

这种结合为解决复杂、高维度的强化学习问题提供了突破性的方法。

## History

...

## Algorithms

存在各种技术来训练策略以使用深度强化学习算法解决任务，每种技术都有其自身优势。

在最高层次上，强化学习算法存在一个重要区分：

1. **模型基础强化学习**(Model-based RL)
   - 算法尝试学习环境动态的前向模型
   - 可以进行规划和模拟
   - 通常更具样本效率

2. **无模型强化学习**(Model-free RL)
   - 直接从与环境交互中学习
   - 不需要显式建模环境动态
   - 通常实现更简单，但可能需要更多样本

这种区分反映了算法是否尝试构建和利用环境的内部模型来预测状态转换和奖励。

在模型基础深度强化学习算法中，系统首先估计环境动态的前向模型，通常采用以下方法：

- 通常使用神经网络通过监督学习来估计环境动态
- 这个模型预测给定当前状态和动作下，下一个状态和奖励会是什么

- 使用学习到的模型进行基于模型的预测控制(model predictive control)
- 通过模拟不同动作序列的结果来选择最优动作

- 由于真实环境动态通常会与学习到的动态模型存在偏差
- 智能体在环境中执行动作时会频繁重新规划
- 这种"重新规划"策略有助于减轻模型误差的影响

- 所选择的动作可以使用蒙特卡洛方法(如交叉熵方法)进行优化
- 或者结合模型学习与无模型方法
- 这种混合方法结合了两种方法的优势

模型基础方法的主要优势在于样本效率，因为学习到的模型可以用来生成无限量的模拟数据。

在无模型深度强化学习算法中，策略 $\pi(a|s)$ 的学习过程不需要显式地建模环境的前向动态。

- 可以通过直接估计策略梯度来优化策略以最大化回报
- 但这种方法存在高方差问题，使其在深度强化学习中与函数近似结合使用时不够实用
- 后续开发了更稳定的学习算法，并得到了广泛应用

- 另一类无模型深度强化学习算法依赖于动态规划
- 受时间差分学习和Q-学习的启发
- 在**离散动作空间**中，这些算法通常学习一个神经网络Q-函数 $Q(s,a)$，用于估计从状态 $s$ 采取动作 $a$ 后的未来回报
- 在**连续空间**中，这些算法通常同时学习值估计和策略

无模型方法的主要优势在于其实现相对简单，不需要准确建模环境，适用于环境动态难以建模的场景。

