# [Reinforcement learning](https://en.wikipedia.org/wiki/Reinforcement_learning)

- [Reinforcement learning](#reinforcement-learning)
  - [Principles](#principles)
  - [Exploration](#exploration)
  - [Algorithms for control learning](#algorithms-for-control-learning)
    - [Criterion of optimality](#criterion-of-optimality)
      - [Policy](#policy)
      - [State-value function](#state-value-function)
    - [Brute force](#brute-force)
    - [Value function](#value-function)
      - [Monte Carlo methods](#monte-carlo-methods)
      - [Temporal difference methods](#temporal-difference-methods)
      - [Function approximation methods](#function-approximation-methods)
    - [Direct policy search](#direct-policy-search)
    - [Model-based algorithms](#model-based-algorithms)
  - [Theory](#theory)

强化学习(RL)是机器学习与最优控制的一个跨学科领域，主要研究智能体如何在动态环境中采取行动以最大化奖励信号。强化学习是三种基本机器学习范式之一，与监督学习和无监督学习并列。

最简单形式的Q-learning将数据存储在表格中。随着状态/动作数量的增加(例如，当状态空间或动作空间为连续时)，这种方法变得不可行，因为智能体访问特定状态并执行特定动作的概率会降低。

强化学习与监督学习的不同之处在于，它不需要提供已标记的输入-输出对，也不需要显式地纠正次优行为。相反，强化学习的重点是在探索(未知领域)和利用(现有知识)之间找到平衡，目标是最大化累积奖励(这种奖励反馈可能是不完整或延迟的)。寻找这种平衡被称为"探索-利用困境"(exploration-exploitation dilemma)。

环境通常以马尔可夫决策过程(Markov decision process, MDP)的形式表述，这是因为许多强化学习算法使用动态规划技术。经典动态规划方法与强化学习算法的主要区别在于：后者不假定已知马尔可夫决策过程的精确数学模型，并且它们针对的是大规模MDP问题，即那些精确方法变得不可行的场景。

## Principles

由于其通用性，强化学习在许多学科中都有研究，例如：
- 博弈论
- 控制论
- 运筹学
- 信息论
- 基于模拟的优化
- 多智能体系统
- 群体智能
- 统计学

在运筹学和控制领域的文献中，强化学习被称为"近似动态规划"(approximate dynamic programming)或"神经动态规划"(neuro-dynamic programming)。强化学习关注的问题也在最优控制理论中被研究，但最优控制理论主要关注最优解的存在性和特性描述，以及计算精确解的算法，较少关注学习或近似(特别是在缺乏环境数学模型的情况下)。

基本的强化学习被建模为一个马尔可夫决策过程，包含：

- 一组环境和智能体状态(状态空间) $\mathcal{S}$；
- 智能体的一组动作(动作空间) $\mathcal{A}$；
- 转移概率 $P_{a}(s,s')=\Pr(S_{t+1}=s'\mid S_{t}=s,A_{t}=a)$，表示在时间 $t$ 下，在动作 $a$ 的影响下从状态 $s$ 转移到状态 $s'$ 的概率；
- 即时奖励 $R_{a}(s,s')$，表示在动作 $a$ 的影响下从状态 $s$ 转移到状态 $s'$ 后获得的即时奖励。

强化学习的目的是使智能体学习最优(或近似最优)策略，以最大化从即时奖励累积而来的奖励函数或其他用户提供的强化信号。这类似于动物心理学中出现的过程。例如，生物大脑天生就将疼痛和饥饿等信号解释为负强化，将愉悦和摄食解释为正强化。在某些情况下，动物会学习采取能优化这些奖励的行为。这表明动物具有强化学习能力。

基本的强化学习智能体以离散时间步与其环境进行交互。在每个时间步 $t$，智能体接收当前状态 $S_t$ 和奖励 $R_t$。然后，它从可用动作集中选择一个动作 $A_t$，并将该动作发送到环境。环境随后转移到新状态 $S_{t+1}$，并确定与转移 $(S_t, A_t, S_{t+1})$ 相关的奖励 $R_{t+1}$。

强化学习智能体的目标是学习一个策略：

$$\pi : \mathcal{S} \times \mathcal{A} \rightarrow [0,1],\quad \pi(s,a) = \Pr(A_t = a \mid S_t = s)$$

该策略能够最大化期望累积奖励。

> 策略公式 $\pi : \mathcal{S} \times \mathcal{A} \rightarrow [0,1],\quad \pi(s,a) = \Pr(A_t = a \mid S_t = s)$ 表示：
> 
> 1. **函数映射关系**：$\pi$ 是一个从状态-动作对到概率值的映射函数，输出值在0到1之间
>
> 2. **概率解释**：$\pi(s,a)$ 表示在当前状态为 $s$ 的条件下，智能体选择动作 $a$ 的概率
> 
> 3. **决策规则**：这个策略本质上是智能体的决策规则，告诉智能体在每个可能的状态下应该如何选择动作
> 
> 这个公式描述的是一个**随机策略**(stochastic policy)，因为它允许智能体在给定状态下以一定的概率分布选择不同的动作。如果策略在每个状态只为一个特定动作分配概率1，而为其他所有动作分配概率0，那么它就是一个**确定性策略**(deterministic policy)。
> 
> 强化学习的核心目标就是找到这样一个最优策略，使得智能体按照该策略行动时能够最大化期望累积奖励。

将问题表述为马尔可夫决策过程时，假设智能体能够直接观察到当前环境状态；这种情况下，问题被称为具有**完全可观测性**(full observability)。

如果智能体只能接触到状态的一个子集，或者观察到的状态被噪声污染，则称智能体具有**部分可观测性**(partial observability)，此时问题必须正式地表述为**部分可观测马尔可夫决策过程**(partially observable Markov decision process, POMDP)。

在这两种情况下，智能体可用的动作集都可能受到限制。例如：
- 账户余额状态可能被限制为只能是正数
- 如果当前状态值为3，而状态转移尝试将该值减少4，则这个转移将不被允许

当智能体的表现与最优行动的智能体相比较时，性能差异产生了**遗憾**(regret)的概念。为了接近最优行动，智能体必须考虑其行为的长期后果(即最大化未来奖励)，尽管与此相关的即时奖励可能是负面的。

因此，强化学习特别适合包含**长期与短期奖励权衡**的问题。它已成功应用于各种领域，包括：

- 能源存储
- 机器人控制
- 光伏发电器
- 西洋双陆棋、跳棋
- 围棋(AlphaGo)
- 自动驾驶系统

两个要素使强化学习变得强大：

1. **使用样本来优化性能**
2. **使用函数近似来处理大型环境**

得益于这两个关键组件，强化学习可以在以下情况下应用于大型环境：

- 环境模型已知，但无法获得分析解；
- 只提供了环境的模拟模型(这是基于模拟的优化的研究对象)[11]；
- 收集环境信息的唯一方法是与之交互。

前两个问题可以被视为**规划问题**(因为某种形式的模型是可用的)，而最后一个问题可以被视为真正的**学习问题**。然而，强化学习将这两类规划问题都转换为机器学习问题。

## Exploration

探索与利用的权衡问题已经通过多臂赌博机问题(multi-armed bandit problem)以及Burnetas和Katehakis(1997)关于有限状态空间马尔可夫决策过程的研究得到了最全面的研究。

强化学习需要巧妙的探索机制；随机选择动作而不参考估计的概率分布，通常表现不佳。对于(小型)有限马尔可夫决策过程的情况，相关理论已经比较完善。然而，由于缺乏能够随状态数量良好扩展的算法(或适用于无限状态空间问题的算法)，简单的探索方法在实践中更为可行。

一种典型方法是**ε-贪心**(ε-greedy)策略，其中 $0<\varepsilon<1$ 是控制探索与利用比例的参数：

- 以概率 $1-\varepsilon$ 选择**利用**，此时智能体选择它认为具有最佳长期效果的动作(如果多个动作同等优秀，则均匀随机打破平局)
- 以概率 $\varepsilon$ 选择**探索**，此时动作是均匀随机选择的

$\varepsilon$ 通常是一个固定参数，但也可以按照一定的计划进行调整(使智能体逐渐减少探索)，或基于启发式方法进行自适应调整。

## Algorithms for control learning

即使不考虑探索问题，也假设状态是可观测的(以下讨论基于此假设)，仍然存在一个关键问题：如何利用过去的经验来确定哪些动作能够带来更高的累积奖励。

### Criterion of optimality

#### Policy

智能体的动作选择被建模为一个称为策略的映射：

$$\pi : \mathcal{A} \times \mathcal{S} \rightarrow [0,1]$$
$$\pi(a,s) = \Pr(A_t = a \mid S_t = s)$$

策略映射给出了在状态 $s$ 时采取动作 $a$ 的概率。

此外还存在**确定性策略** $\pi$，其中 $\pi(s)$ 表示在状态 $s$ 下应该执行的确定动作。

#### State-value function

状态值函数 $V_\pi(s)$ 被定义为：从状态 $s$ 开始(即 $S_0=s$)，并持续按照策略 $\pi$ 行动所能获得的期望折现回报。简而言之，值函数评估了处于特定状态的"好坏程度"。

$$V_\pi(s) = \mathbb{E}[G|S_0=s] = \mathbb{E}\left[\sum_{t=0}^{\infty}\gamma^t R_{t+1}|S_0=s\right]$$

其中，随机变量 $G$ 表示**折现回报**(discounted return)，定义为未来折现奖励的总和：

$$G = \sum_{t=0}^{\infty}\gamma^t R_{t+1} = R_1 + \gamma R_2 + \gamma^2 R_3 + \dots$$

这里：
- $R_{t+1}$ 是从状态 $S_t$ 转移到 $S_{t+1}$ 时获得的奖励
- $0 \leq \gamma < 1$ 是折现率
- 由于 $\gamma$ 小于1，所以远期未来的奖励比即时奖励的权重要小

算法必须找到一个具有**最大期望折现回报**的策略。根据马尔可夫决策过程理论，我们知道，在不失一般性的前提下，搜索范围可以限制在所谓的**平稳策略**(stationary policies)集合中。

如果策略返回的动作分布仅依赖于最后访问的状态(基于智能体观察历史)，则该策略被称为平稳的。搜索范围可以进一步限制在**确定性平稳策略**(deterministic stationary policies)中。确定性平稳策略根据当前状态确定性地选择动作。

由于任何此类策略都可以用从状态集到动作集的映射来表示，因此这些策略可以在不失一般性的情况下与此类映射等同。

### Brute force

暴力方法包含两个步骤：

1. 对于每个可能的策略，采样其执行过程中的回报
2. 选择具有最大期望折现回报的策略

这种方法面临的问题有：
- 策略数量可能非常大，甚至是无限的
- 回报的方差可能很大，这需要大量样本才能准确估计每个策略的折现回报

如果我们假设某种结构，并允许从一个策略生成的样本影响对其他策略的估计，这些问题可以得到缓解。实现这一目标的两种主要方法是：

1. **值函数估计**(value function estimation)
2. **直接策略搜索**(direct policy search)

### Value function

值函数方法试图找到一个能够最大化折现回报的策略，方法是维护一组期望折现回报 $\mathbb{E}[G]$ 的估计值，这些估计通常针对：

- "当前"策略(on-policy，在线策略)
- 或最优策略(off-policy，离线策略)

这些方法依赖于马尔可夫决策过程理论，其中最优性的定义比上述更强：如果一个策略从任何初始状态都能获得最佳期望折现回报，则该策略被认为是最优的(即初始分布在这个定义中不起作用)。同样，最优策略总是可以在平稳策略中找到。

策略 $\pi$ 的状态值函数定义为：

$$V^\pi(s) = \mathbb{E}[G|s,\pi]$$

其中 $G$ 表示从初始状态 $s$ 开始，按照策略 $\pi$ 行动所获得的折现回报。

定义 $V^*(s)$ 为 $V^\pi(s)$ 的最大可能值，其中允许 $\pi$ 变化：

$$V^*(s) = \max_\pi V^\pi(s)$$

在每个状态下都能达到这些最优状态值的策略被称为**最优策略**。显然，在这个意义上最优的策略也是在最大化期望折现回报意义上的最优，因为：

$$V^*(s) = \max_\pi \mathbb{E}[G|s,\pi]$$

其中 $s$ 是从初始状态分布 $\mu$ 中随机采样的状态(所以 $\mu(s) = \Pr(S_0 = s)$)。

尽管状态值足以定义最优性，但定义动作值也很有用。给定状态 $s$、动作 $a$ 和策略 $\pi$，在策略 $\pi$ 下，状态-动作对 $(s,a)$ 的动作值定义为：

$$Q^\pi(s,a) = \mathbb{E}[G|s,a,\pi]$$

其中 $G$ 表示在状态 $s$ 下首先采取动作 $a$，然后遵循策略 $\pi$ 所获得的随机折现回报。

马尔可夫决策过程理论指出，如果 $\pi^*$ 是最优策略，我们可以在每个状态 $s$ 通过从 $Q^{\pi^*}(s,\cdot)$ 中选择具有最高动作值的动作来实现最优行动。这样的最优策略的动作值函数($Q^{\pi^*}$)被称为**最优动作值函数**，通常表示为 $Q^*$。总之，仅仅知道最优动作值函数就足以知道如何最优地行动。

假设我们完全了解马尔可夫决策过程，计算最优动作值函数的两种基本方法是：

1. **值迭代**(value iteration)
2. **策略迭代**(policy iteration)

这两种算法都计算一系列函数 $Q_k$ ($k=0,1,2,\ldots$)，它们会收敛到 $Q^*$。计算这些函数涉及对整个状态空间进行期望计算，这对于除了最小的(有限)马尔可夫决策过程外，都是不切实际的。

在强化学习方法中，期望计算通过以下方式近似：
- 对样本进行平均
- 使用函数近似技术来处理在大型状态-动作空间上表示值函数的需求

#### Monte Carlo methods

蒙特卡洛方法通过对样本回报求平均来解决强化学习问题。与需要完全了解环境动态的方法不同，蒙特卡洛方法仅依赖于：
- 实际经验
- 或模拟经验

这些经验是从与环境交互中获得的**状态、动作和奖励序列**。

这种方法的优势包括：

- 适用于完整环境动态未知的情况
- 从实际经验学习不需要环境的先验知识，但仍能导致最优行为
- 使用模拟经验时，只需要一个能够生成样本转换的模型，而不需要完整的转换概率规范(这是动态规划方法所必需的)

蒙特卡洛方法适用于**情节性任务**(episodic tasks)，在这类任务中，经验被划分为最终会结束的情节(episodes)。蒙特卡洛方法具有以下特点：

- **更新时机**：策略和值函数的更新仅在情节完成后进行
- **递增性质**：这些方法是以情节为单位递增的，而非以步骤为单位(在线)递增
- **术语含义**："蒙特卡洛"一词通常指代任何涉及随机采样的方法，但在强化学习的具体语境中，它特指从完整回报(而非部分回报)计算平均值的方法

这些方法的运作方式与老虎机(bandit)算法类似，即对每个状态-动作对的回报进行平均。但存在一个关键差异：

- 在一个状态中采取的动作会影响同一情节中后续状态的回报，使问题变成非平稳的(non-stationary)

为了解决这种非平稳性，蒙特卡洛方法使用**通用策略迭代**(General Policy Iteration, GPI)框架。

- **动态规划**：使用对马尔可夫决策过程(MDP)的完全了解来计算值函数
- **蒙特卡洛方法**：通过样本回报学习这些函数

值函数和策略的交互方式与动态规划类似，以实现最优性：
1. 首先解决预测问题
2. 然后扩展到策略改进和控制

所有这些都基于采样得到的经验。

#### Temporal difference methods

第一个问题(值函数需要完全收敛)可以通过允许程序在值函数完全稳定之前就改变策略(在部分或全部状态)来解决。然而，这种方法也可能存在问题，因为它可能会阻止算法收敛。

尽管如此，大多数当前的算法都采用这种方法，形成了**广义策略迭代**(generalized policy iteration)算法类别。许多**行动者-评论家方法**(actor-critic methods)都属于这一类别。

第二个问题(样本效率)可以通过允许轨迹对其中的任何状态-动作对做出贡献来纠正。这也可能在某种程度上帮助解决第三个问题，但对于**高方差回报**的更好解决方案是Sutton提出的**时间差分(TD)方法**，该方法基于递归贝尔曼方程。

时间差分方法的计算可以分为两种类型：

1. **增量式**(incremental)：在每次转换后修改内存并丢弃转换
2. **批处理式**(batch)：转换被批量收集，估计基于整个批次一次性计算

- **批处理方法**(如最小二乘时间差分方法)可能更好地利用样本中的信息
- **增量方法**在批处理方法因高计算或内存复杂度而不可行时是唯一的选择
- 一些方法尝试结合这两种方法的优势

基于时间差分的方法还克服了第四个问题。

TD方法由于依赖递归贝尔曼方程而产生特定问题。为了解决这一问题，大多数TD方法引入了一个称为 $\lambda$ 的参数(其中 $0 \leq \lambda \leq 1$)，该参数可以在以下两者之间实现连续插值：

- **蒙特卡洛方法**：不依赖贝尔曼方程
- **基础TD方法**：完全依赖贝尔曼方程

这种参数化方法在缓解TD方法依赖递归贝尔曼方程所带来的问题方面非常有效。

#### Function approximation methods

为了解决第五个问题，研究者采用**函数近似方法**。

线性函数近似首先定义一个映射 $\phi$，将每个状态-动作对映射到一个有限维向量。然后，通过线性组合 $\phi(s,a)$ 的分量与某些权重 $\theta$ 来获得状态-动作对 $(s,a)$ 的动作值：

$$Q(s,a) = \sum_{i=1}^{d} \theta_i \phi_i(s,a)$$

这些算法调整的是权重参数，而不是调整与单个状态-动作对相关的值。这种方法显著减少了需要学习的参数数量。

基于非参数统计学思想的方法(可以被视为构建自己的特征)也已被探索。这类方法能够自动提取相关特征，进一步提高函数近似的效果。

值迭代也可以作为起点，衍生出**Q-learning算法**及其许多变体。当使用神经网络来表示Q值时，这些方法被称为**深度Q-learning方法**(Deep Q-learning)，并在随机搜索问题中有各种应用。

使用动作值的问题在于：在回报含有噪声的情况下，可能需要对竞争动作的值进行高精度估计，这往往很难获得。不过，时间差分方法在一定程度上缓解了这个问题。

使用所谓的"兼容函数近似方法"(compatible function approximation method)虽然能解决某些问题，但会在通用性和效率之间做出折中。

### Direct policy search

另一种方法是直接在策略空间(或其子集)中搜索，这种情况下问题变成了一个**随机优化**问题。可用的两种方法是：

1. 基于梯度的方法
2. 无梯度的方法

**策略梯度方法**(Policy gradient methods)从有限维(参数)空间到策略空间的映射开始：给定参数向量 $\theta$，用 $\pi_\theta$ 表示与 $\theta$ 相关联的策略。性能函数定义为 $\rho(\theta) = \rho^{\pi_\theta}$，在温和条件下，这个函数作为参数向量 $\theta$ 的函数是可微的。

- 如果已知 $\rho$ 的梯度，就可以使用**梯度上升**
- 由于无法获得梯度的解析表达式，只能使用带噪声的估计
- 这种估计可通过多种方式构建，产生了诸如Williams的**REINFORCE方法**等算法(在基于模拟的优化文献中被称为似然比方法)

一大类方法避免依赖梯度信息，包括：
- 模拟退火(simulated annealing)
- 交叉熵搜索(cross-entropy search)
- 进化计算方法(evolutionary computation)

许多无梯度方法可以(在理论上和极限情况下)达到全局最优。

- **收敛速度**：策略搜索方法在面对噪声数据时可能收敛缓慢，特别是在长轨迹且回报方差大的情况下
- **改进方向**：基于时间差分的值函数方法可能在这种情况下有所帮助
- **最新进展**：近年来，**行动者-评论家方法**(actor-critic methods)表现良好
- **应用**：策略搜索方法已应用于机器人领域
- **局限性**：许多策略搜索方法可能陷入局部最优(因为基于局部搜索)

### Model-based algorithms

以上所有方法都可以与首先学习马尔可夫决策过程模型的算法结合使用，该模型预测从现有状态采取行动后各个可能的下一状态的概率。

例如，**Dyna算法**从经验中学习模型，并使用该模型为值函数提供更多的模拟转换，这些模拟转换是对真实转换的补充。 

这类方法有时可以扩展到使用非参数模型，例如当转换简单地被存储并"重放"给学习算法时。

- **计算复杂性**：模型基础方法可能比无模型方法计算强度更高
- **实用性限制**：其实用性可能受限于马尔可夫决策过程可学习的程度

使用模型的方式不仅限于更新值函数。例如，在**模型预测控制**(model predictive control)中，模型被用来直接更新行为。

## Theory

大多数强化学习算法的渐近行为和有限样本行为都已经得到了很好的理解。存在具有可证明的良好在线性能(解决探索问题)的算法。

Burnetas和Katehakis(1997)提出了马尔可夫决策过程的有效探索方法。许多算法也已经有了有限时间性能界限，但这些界限预计会相当宽松，因此需要更多工作来更好地理解各种算法的相对优势和局限性。

对于增量算法，渐近收敛问题已经解决。基于时间差分的算法在比以前可能的更广泛的条件下收敛(例如，当与任意平滑函数近似一起使用时)。
