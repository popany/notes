# Policy

- [Policy](#policy)

---

## 强化学习中的策略 (Policy)

在强化学习 (Reinforcement Learning, RL) 中，**策略 (Policy)** 是智能体 (Agent) 的**行为准则**，它定义了在给定状态 (State) 下，智能体应该采取什么样的**动作 (Action)**。 简单来说，策略就是智能体的**大脑**或者**决策函数**，指导着智能体与环境的交互方式。

更具体地来说，策略可以理解为**从状态到动作的映射**。  它可以是：

* **确定性策略 (Deterministic Policy):**  对于每个状态 $s$，策略会**唯一确定**一个动作 $a$。  我们可以用函数表示为：
   $$ a = \pi(s) $$
   其中，$\pi$ 表示策略函数，$s$是当前状态，$a$是策略在状态 $s$下选择的动作。

* **随机性策略 (Stochastic Policy):** 对于每个状态 $s$，策略会给出一个**动作的概率分布**。  智能体会根据这个概率分布来**随机选择**一个动作。 我们可以用条件概率表示为：
   $$ \pi(a|s) = P(A=a | S=s) $$
   其中，$\pi(a|s)$表示在状态 $s$下选择动作 $a$的概率。 $A$和 $S$分别表示动作和状态的随机变量。

**为什么需要随机性策略？**

在很多情况下，使用随机性策略比确定性策略更有效，尤其是在探索 (Exploration) 方面：

* **鼓励探索:** 随机性策略允许智能体在同一个状态下尝试不同的动作，有助于探索更广阔的状态空间，发现潜在的最优策略。
* **应对环境不确定性:**  在环境存在噪声或不确定性的情况下，随机性策略可以更加鲁棒。
* **避免陷入局部最优:**  确定性策略容易陷入局部最优解，而随机性策略有机会跳出局部最优，找到全局最优解。

## 在线策略 (On-policy) 和 离线策略 (Off-policy)

在线策略和离线策略是强化学习中两种重要的学习方式，它们的核心区别在于**生成样本数据的策略**和**学习优化的策略**是否是**同一个策略**。

**1. 在线策略 (On-policy)**

* **概念:** 在线策略学习是指 **学习的策略** 和 **与环境交互生成样本数据的策略** 是 **同一个策略**。  智能体使用当前正在学习的策略与环境交互，收集经验数据，然后用这些数据来改进**同一个策略**。

* **学习过程:**
    1. 智能体使用当前策略 $\pi $与环境交互，生成一系列的 **状态-动作-奖励-下一个状态 (s, a, r, s')** 的样本数据。
    2. 使用这些样本数据来**评估**和**改进**策略 $\pi$。
    3. 重复步骤 1 和 2，不断迭代优化策略 $\pi$。

* **形象比喻:**  就像 **边学边做**，或者 **在实践中学习**。  你根据自己当前的学习方法 (策略) 去实践，然后根据实践的反馈来改进你的学习方法。

* **典型算法:**  Sarsa,  A2C, A3C,  Policy Gradient 方法 (如 REINFORCE, PPO, TRPO) 等。

**2. 离线策略 (Off-policy)**

* **概念:** 离线策略学习是指 **学习的策略** 和 **与环境交互生成样本数据的策略** 是 **不同的策略**。  智能体可以利用**其他策略** (可以是过去的策略，甚至是人类专家的策略，或者随机策略) 生成的数据来学习和改进**自己的策略**。

* **学习过程:**
    1. 智能体使用一个 **行为策略 (Behavior Policy)** $\mu$与环境交互，生成样本数据。这个$\mu$可以是任意策略，甚至可以是多个策略的混合。
    2. 使用 **行为策略** $\mu$生成的样本数据来**评估**和**改进** **目标策略 (Target Policy)** $\pi$。目标策略 $\pi$是我们最终想要学习和优化的策略。
    3. 目标策略 $\pi$ **不直接与环境交互** 生成数据，而是利用 **行为策略** $\mu$的数据进行学习。

* **形象比喻:**  就像 **看别人做，自己学习**，或者 **从别人的经验中学习**。  你观察别人 (行为策略) 的行为和结果，然后学习并改进自己的方法 (目标策略)，但你并不需要亲自去实践所有的步骤。

* **典型算法:**  Q-Learning, Deep Q-Network (DQN),  Deep Deterministic Policy Gradient (DDPG),  Soft Actor-Critic (SAC) 等。  这些算法通常会使用 **经验回放 (Experience Replay)** 技术，将行为策略生成的数据存储起来，然后从中采样进行学习。

**总结在线策略和离线策略的区别:**

| 特征          | 在线策略 (On-policy)                                  | 离线策略 (Off-policy)                                  |
|---------------|----------------------------------------------------|----------------------------------------------------|
| **数据来源**     | 当前策略 $\pi$与环境交互生成的数据                        | 其他策略 (行为策略 $\mu$) 与环境交互生成的数据               |
| **学习策略与数据生成策略** | 同一个策略                                          | 不同的策略                                          |
| **数据利用效率**  | 通常较低，因为每次更新策略后，之前的数据可能就不是最优的了。               | 通常较高，可以重复利用历史数据，数据效率更高。                        |
| **探索-利用平衡** | 通常更直接地平衡探索和利用，因为学习策略就是执行策略。                  | 探索和利用可以解耦，行为策略负责探索，目标策略负责利用和优化。            |
| **算法复杂度**   | 有些在线策略算法相对简单，例如 Sarsa, Policy Gradient。             | 有些离线策略算法可能更复杂，例如 Q-Learning, DQN。                |
| **适用场景**     | 适合需要直接评估当前策略性能的场景，例如策略梯度算法。                  | 适合数据获取成本较高，或者可以利用历史数据或他人数据的场景，例如模拟器环境。 |

**选择在线策略还是离线策略，取决于具体的应用场景和需求。**  例如，如果环境交互成本很高，希望尽可能利用已有的数据，那么离线策略可能更合适。 如果需要直接评估当前策略的性能，并且环境交互成本可以接受，那么在线策略可能更直接有效。
